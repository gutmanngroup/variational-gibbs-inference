{
    "model_seed": 20190508,
    "data_seeds": [20200325, 20200406, 20200407],
    "exp_group": "flows_uci/learning_experiments/1",
    "experiment_name": "rqcspline_power_plmcmc",
    "save_custom_epochs": [2, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120],
    "data": {
        "dataset": "uci_power",
        "num_imputed_copies": [5],
        "total_miss": 0.1667,
        "miss_type": "MCAR",
        "filter_fully_missing": true,
        "pre_imputation": "empirical_distribution_samples",
        "batch_size": 512
    },
    "method": "plmcmc",
    "plmcmc": {
        "mcmc_before_epoch": true,
        "mcmc_during_epoch": false,
        "mcmc_batch_size": 400000,
        "dim": 6,
        "resample_prop_prob": 0.5,
        "resample_prop_std": 1.0,
        "perturb_prop_std": 0.01,
        // Don't perturb observed data
        "perturb_std": 0.0,
        "aux_dist_std": 0.001,
        "clamp_imputations": true,

        "approximate_kernel": false,

        "num_imp_steps_schedule": [0, 3, 25, 60],
        "num_imp_steps_schedule_values": [0, 200, 500, 1000],
        // "num_imp_steps_schedule": [0, 40, 350, 600],
        // "num_imp_steps_schedule_values": [0, 500, 2000, 3000],

        "latent_reset_schedule": [0, 3],
        "latent_reset_schedule_values": ["USE_GAUSS", "USE_MODEL"],

        "update_imputations_schedule_init_value": true,
        "update_imputations_schedule_main_value": true,
        "update_imputations_schedule_other_value": false,
        "update_imputations_schedule_start_epoch": 3,
        "update_imputations_schedule_period": 7
    },
    "density_model": "flow",
    "flow": {
        "dim": 6,
        "base_transform_type": "rq-coupling",
        "linear_transform_type": "lu",
        "num_flow_steps": 10,
        "hidden_features": 256,
        "tail_bound": 3,
        "num_bins": 8,
        "num_transform_blocks": 2,
        "use_batch_norm": false,
        "dropout_probability": 0.0,
        "apply_unconditional_transform": true
    },
    //
    // Trainer parameters
    //
    // "max_epochs": 127,
    "max_epochs": 110,
    // "max_epochs": 95,
    "model_optim": {
        "optimiser": "adam",
        "learning_rate": 5e-4,
        "weight_decay_coeff": 0.0,
        "anneal_learning_rate": true
    }
}
