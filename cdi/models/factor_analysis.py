import torch
import torch.distributions as distr
import pytorch_lightning as pl
import numpy as np  # TODO: remove dependence on np

from cdi.util.utils import set_random


class FactorAnalysis(pl.LightningModule):
    """
    Factor Analysis model that will be trained using SGD on incomplete data.
    """
    def __init__(self, args):
        super(FactorAnalysis, self).__init__()
        self.args = args.fa_model

        self.init_parameters()
        self.cum_batch_size_called = 0

    @staticmethod
    def add_model_args(parser):
        parser.add_argument('--fa_model.input_dim', type=int,
                            help='Observed dimensionality.')
        parser.add_argument('--fa_model.latent_dim', type=int,
                            help='Latent dimensionality.')
        return parser

    def init_parameters(self):
        self.factor_loadings = torch.nn.Parameter(
                                            data=torch.randn(
                                                    self.args.input_dim,
                                                    self.args.latent_dim,
                                                    dtype=torch.float),
                                            requires_grad=True)
        self.mean = torch.nn.Parameter(torch.zeros(self.args.input_dim,
                                       dtype=torch.float),
                                       requires_grad=True)
        # Diagonal log-covariance component,
        # since covariance has to be positive.
        self.log_cov = torch.nn.Parameter(torch.ones(self.args.input_dim,
                                                     dtype=torch.float),
                                          requires_grad=True)

    def set_parameters(self, F, mean, log_cov):
        self.factor_loadings = torch.nn.Parameter(
                                            data=F,
                                            requires_grad=True)
        self.mean = torch.nn.Parameter(mean,
                                       requires_grad=True)
        # Diagonal log-covariance component,
        # since covariance has to be positive.
        self.log_cov = torch.nn.Parameter(log_cov,
                                          requires_grad=True)

    def forward(self, X, M):
        # Track training calls
        if self.training:
            self.cum_batch_size_called += X.shape[0]
        return self.log_prob(X)

    def log_prob(self, X):
        """
        Computes average log-prob that X were generated by this model.
        Args:
            X (N, D): observable variable batch
        Returns:
            avg_log_prob (float): average log-prob of X across the batch.
        """
        mean, marginal_cov = self.observable_marginal_parameters()
        multi_norm = distr.MultivariateNormal(loc=mean,
                                              covariance_matrix=marginal_cov)

        # Compute the log probability of samples in X
        return multi_norm.log_prob(X)

    def log_marginal_prob(self, X, M):
        """
        Computes the marginal log-prob of observed X as indicated by M
        """
        M_not = ~M

        F_obs = M.unsqueeze(-1) * self.factor_loadings
        psi_obs = torch.exp(self.log_cov).repeat(X.shape[0], 1)
        psi_obs[~M] = 0
        cov_obs = F_obs @ F_obs.transpose(1, 2) + torch.diag_embed(psi_obs)

        # Compute the determinant term contribution to log-prob
        cov_det = ((2 * np.pi) * cov_obs)
        M_not_diag = torch.diag_embed(M_not)
        cov_det = cov_det.masked_fill(M_not_diag, 1)

        log_prob = -1/2 * torch.logdet(cov_det)

        # Compute the square term contribution to log-prob
        cov_obs = cov_obs.masked_fill(M_not_diag, 1)
        d = (X - self.mean) * M
        log_prob += (-1/2 * d.unsqueeze(1)
                     @ torch.inverse(cov_obs)
                     @ d.unsqueeze(-1)).squeeze()

        return log_prob

    def observable_marginal_parameters(self):
        """
        Computes the observable variable distribution parameters.
        Returns:
            mean (D): mean of the observables
            marginal_cov (D, D): covariance of the observables
        """
        # Compute the marginal distribution parameters
        # (the latents are independent unit Gaussian)
        cov = torch.exp(self.log_cov)
        marginal_cov = torch.matmul(self.factor_loadings,
                                    self.factor_loadings.t())
        marginal_cov += torch.diag(cov)

        return self.mean, marginal_cov

    def reset_parameters(self):
        self.init_parameters()

    def on_epoch_start(self):
        self.cum_batch_size_called = 0

    # Misc

    def generate_data(self, N, seed=None):
        if seed is not None:
            set_random(seed=seed)

        mean, cov = self.observable_marginal_parameters()
        multi_gauss = distr.MultivariateNormal(loc=mean, covariance_matrix=cov)

        return multi_gauss.sample(sample_shape=(N,))
